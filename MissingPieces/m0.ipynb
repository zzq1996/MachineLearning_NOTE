{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 经验风险与结构风险\n",
    "\n",
    "非常好，这两个概念是理解**机器学习泛化能力与正则化方法**的理论基础，来自**统计学习理论（Statistical Learning Theory）**，特别是 Vapnik 的“VC理论”。\n",
    "\n",
    "---\n",
    "\n",
    "# 一、经验风险（Empirical Risk）\n",
    "\n",
    "### 定义（英文：**Empirical Risk**）\n",
    "\n",
    "* **经验风险**是指：在训练样本上计算得到的平均损失。\n",
    "* 它是模型在**训练集**上的性能评价指标。\n",
    "\n",
    "$$\n",
    "R_{emp}(f) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathcal{L}(f(x_i), y_i)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $n$：训练样本数量；\n",
    "* $f(x_i)$：模型对样本 $x_i$ 的预测；\n",
    "* $y_i$：对应的真实标签；\n",
    "* $\\mathcal{L}$：损失函数（如均方误差、交叉熵等）。\n",
    "\n",
    "### 直观理解：\n",
    "\n",
    "> 经验风险是“模型在训练集上的平均犯错程度”，即模型对已知数据的拟合程度。\n",
    "\n",
    "---\n",
    "\n",
    "# 二、结构风险（Structural Risk）\n",
    "\n",
    "### 定义（英文：**Structural Risk** 或 **Regularized Risk**）\n",
    "\n",
    "* **结构风险**是在经验风险的基础上，**加入对模型复杂度的惩罚项**（即正则化项）：\n",
    "\n",
    "$$\n",
    "R_{struct}(f) = R_{emp}(f) + \\lambda \\cdot \\Omega(f)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $R_{emp}(f)$：经验风险；\n",
    "* $\\Omega(f)$：模型复杂度（如权重范数、模型容量等）；\n",
    "* $\\lambda$：正则化系数，控制惩罚项的权重。\n",
    "\n",
    "### 举例：L2 正则化\n",
    "\n",
    "$$\n",
    "\\Omega(f) = \\|w\\|_2^2\n",
    "$$\n",
    "\n",
    "则结构风险就是：\n",
    "\n",
    "$$\n",
    "R_{struct}(f) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathcal{L}(f(x_i), y_i) + \\lambda \\|w\\|_2^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# 三、经验风险最小化 vs. 结构风险最小化\n",
    "\n",
    "| 原则               | 解释                        | 问题                |\n",
    "| ---------------- | ------------------------- | ----------------- |\n",
    "| **经验风险最小化（ERM）** | 只最小化训练集上的损失               | 易过拟合（overfitting） |\n",
    "| **结构风险最小化（SRM）** | 在最小化训练损失的同时控制模型复杂度，提升泛化能力 | 需选择好正则项和参数        |\n",
    "\n",
    "> **经验风险最小化（ERM, Empirical Risk Minimization）** 是最早的学习原则，但它在训练数据量少时容易过拟合；\n",
    ">\n",
    "> **结构风险最小化（SRM, Structural Risk Minimization）** 是对 ERM 的改进，考虑了模型复杂度，从而更能提高模型对新样本的泛化能力。\n",
    "\n",
    "---\n",
    "\n",
    "# 四、联系与区别总结\n",
    "\n",
    "| 概念       | 经验风险（Empirical Risk） | 结构风险（Structural Risk）                  |\n",
    "| -------- | -------------------- | -------------------------------------- |\n",
    "| 计算内容     | 训练集上的平均损失            | 训练集损失 + 模型复杂度惩罚项                       |\n",
    "| 目标       | 拟合训练数据               | 拟合训练数据 + 控制模型复杂度（防止过拟合）                |\n",
    "| 是否包含正则项  | 否                    | 是                                      |\n",
    "| 是否考虑泛化能力 | 否                    | 是                                      |\n",
    "| 常见实现方式   | 最小化损失函数              | 加上 L1/L2 正则、Dropout、Early Stopping 等技术 |\n",
    "\n",
    "---\n",
    "\n",
    "# 五、总结笔记版本（推荐）\n",
    "\n",
    "> 在机器学习中，模型的最终目标是拥有良好的**泛化能力（generalization）**，即在新数据上表现良好。\n",
    ">\n",
    "> * **经验风险（Empirical Risk）**：是模型在训练集上的平均损失，是经验风险最小化（ERM）原则的核心。\n",
    "> * **结构风险（Structural Risk）**：在经验风险的基础上加入模型复杂度的惩罚项，防止模型过拟合，是结构风险最小化（SRM）原则的核心。\n",
    ">\n",
    "> 结构风险通过正则化、模型简化等方式约束模型能力，达到泛化能力与训练性能之间的平衡，是现代机器学习的核心思想之一。\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ],
   "id": "45e361e2337f5f5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 方差与偏差\n",
    "\n",
    " Bias-Variance Decomposition（偏差-方差分解）和Bias-Variance Dilemma（偏差-方差困境）是统计学习理论中解释模型泛化误差的重要工具，特别适合理解欠拟合（underfitting）和过拟合（overfitting）的本质。\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# 一、什么是 Bias-Variance Decomposition（偏差-方差分解）\n",
    "\n",
    "我们希望学习一个模型 $\\hat{f}(x)$ 来逼近真实的数据分布 $f(x)$。但是由于模型有限、训练集有限，最终预测会产生误差。\n",
    "\n",
    "\n",
    "## 偏差-方差分解的数学推导（Bias–Variance Decomposition）\n",
    "\n",
    "我们要研究的是预测函数的期望平方误差（Mean Squared Error, MSE）：\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{D, \\varepsilon} \\left[ \\left( \\hat{f}(x; D) - y \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $x$：输入；\n",
    "* $y = f(x) + \\varepsilon$：真实目标，$f(x)$ 为真实函数，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 为不可约噪声；\n",
    "* $D$：从训练集分布中采样得到的数据集；\n",
    "* $\\hat{f}(x; D)$：模型从训练集 $D$ 学到的预测函数。\n",
    "\n",
    "我们将对以下期望误差进行分解：\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{D, \\varepsilon} \\left[ \\left( \\hat{f}(x; D) - y \\right)^2 \\right] = ?\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 第一步：展开平方项\n",
    "\n",
    "由于 $y = f(x) + \\varepsilon$，代入得到：\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{D, \\varepsilon} \\left[ \\left( \\hat{f}(x; D) - f(x) - \\varepsilon \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "展开平方项：\n",
    "\n",
    "$$\n",
    "= \\mathbb{E}_{D, \\varepsilon} \\left[ \\left( \\hat{f}(x; D) - f(x) \\right)^2 - 2\\varepsilon(\\hat{f}(x; D) - f(x)) + \\varepsilon^2 \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 第二步：对噪声 $\\varepsilon$ 求期望\n",
    "\n",
    "因为：\n",
    "\n",
    "* $\\mathbb{E}[\\varepsilon] = 0$\n",
    "* $\\mathbb{E}[\\varepsilon^2] = \\sigma^2$\n",
    "* $\\varepsilon$ 与 $\\hat{f}(x; D)$ 独立\n",
    "\n",
    "所以有：\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\varepsilon}[-2\\varepsilon(\\hat{f}(x; D) - f(x))] = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\varepsilon}[\\varepsilon^2] = \\sigma^2\n",
    "$$\n",
    "\n",
    "因此原式简化为：\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{D} \\left[ \\left( \\hat{f}(x; D) - f(x) \\right)^2 \\right] + \\sigma^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 第三步：对模型误差部分再分解\n",
    "\n",
    "我们现在看：\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{D} \\left[ \\left( \\hat{f}(x; D) - f(x) \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "这个期望平方误差可以分解为：\n",
    "\n",
    "$$\n",
    "\\underbrace{\\left( \\mathbb{E}_D[\\hat{f}(x; D)] - f(x) \\right)^2}_{\\text{Bias}^2} + \\underbrace{\\mathbb{E}_D \\left[ \\left( \\hat{f}(x; D) - \\mathbb{E}_D[\\hat{f}(x; D)] \\right)^2 \\right]}_{\\text{Variance}}\n",
    "$$\n",
    "\n",
    "这是一个标准恒等式（方差展开公式）：\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(X - a)^2] = (\\mathbb{E}[X] - a)^2 + \\text{Var}(X)\n",
    "\\quad\\text{设 } a = f(x)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 第四步：最终公式总结\n",
    "\n",
    "因此，模型在某个点 $x$ 上的总期望平方误差可以分解为：\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\mathbb{E}_{D, \\varepsilon} \\left[ \\left( \\hat{f}(x; D) - y \\right)^2 \\right]\n",
    "=\n",
    "\\underbrace{\\left( \\mathbb{E}_D[\\hat{f}(x; D)] - f(x) \\right)^2}_{\\text{Bias}^2}\n",
    "+\n",
    "\\underbrace{\\mathbb{E}_D \\left[ \\left( \\hat{f}(x; D) - \\mathbb{E}_D[\\hat{f}(x; D)] \\right)^2 \\right]}_{\\text{Variance}}\n",
    "+\n",
    "\\underbrace{\\sigma^2}_{\\text{Irreducible Error}}\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# 偏差-方差分解笔记整理（推荐格式）\n",
    "\n",
    "## 偏差-方差分解（Bias–Variance Decomposition）\n",
    "\n",
    "目标：分析模型预测误差的来源\n",
    "\n",
    "**总误差 = 偏差² + 方差 + 不可约误差**\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{D, \\varepsilon}[(\\hat{f}(x) - y)^2]\n",
    "=\n",
    "\\underbrace{(\\mathbb{E}[\\hat{f}(x)] - f(x))^2}_{\\text{Bias}^2}\n",
    "+\n",
    "\\underbrace{\\mathbb{E}[(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)])^2]}_{\\text{Variance}}\n",
    "+\n",
    "\\underbrace{\\mathbb{E}[\\varepsilon^2]}_{\\text{Irreducible Error}}\n",
    "$$\n",
    "\n",
    "### 三部分解释：\n",
    "\n",
    "* **Bias²（偏差平方）**：模型预测的期望与真实值的偏离程度 ⇒ 欠拟合风险；\n",
    "* **Variance（方差）**：模型预测对训练集变化的敏感度 ⇒ 过拟合风险；\n",
    "* **Irreducible Error（不可约误差）**：标签噪声，无法通过学习降低；\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Bias²（偏差平方）\n",
    "\n",
    "* 指的是**模型的平均预测值与真实目标值之间的差异**。\n",
    "* 偏差大 ⇒ 模型能力弱、无法学到数据本质 ⇒ **欠拟合**。\n",
    "* 常见于线性模型、过于简单的模型。\n",
    "\n",
    "### 2. Variance（方差）\n",
    "\n",
    "* 指的是**同一个输入在不同训练集下模型预测的波动性**。\n",
    "* 方差大 ⇒ 模型对训练数据太敏感 ⇒ **过拟合**。\n",
    "* 常见于复杂模型、小训练集下的神经网络。\n",
    "\n",
    "### 3. Irreducible Error（不可约误差）\n",
    "\n",
    "* 是由于**数据本身的噪声**或标签误差导致的，即便你有完美模型也无法消除。\n",
    "\n",
    "---\n",
    "\n",
    "## 图像化理解（打靶图示意）\n",
    "\n",
    "* 偏差大 = 平均打得很偏，远离目标；\n",
    "* 方差大 = 每次打得很分散，波动大；\n",
    "* 最优情况 = 偏差小、方差小，集中在目标附近。\n",
    "\n",
    "```\n",
    "        目标 ← True Function\n",
    "        O ←←←\n",
    "  ↘            ↙\n",
    "x   x   x   x   x    ← 方差大，偏差小（过拟合）\n",
    "          x\n",
    "          x           ← 偏差大，方差小（欠拟合）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 二、Bias-Variance Dilemma（偏差-方差困境）\n",
    "\n",
    "### 定义：\n",
    "\n",
    "> 当我们试图减少模型的**偏差**时，往往会导致**方差上升**；而当我们降低**方差**时，又可能会增大**偏差**。\n",
    "\n",
    "这就是所谓的**偏差-方差困境（Bias-Variance Trade-off）**。\n",
    "\n",
    "---\n",
    "\n",
    "## 举例说明：\n",
    "\n",
    "| 模型类型    | 偏差（Bias） | 方差（Variance） | 总体表现    |\n",
    "| ------- | -------- | ------------ | ------- |\n",
    "| 线性回归    | 高        | 低            | 容易欠拟合   |\n",
    "| 高阶多项式回归 | 低        | 高            | 容易过拟合   |\n",
    "| 正则化模型   | 中等       | 中等           | 偏差和方差均衡 |\n",
    "\n",
    "---\n",
    "\n",
    "## 如何应对困境：\n",
    "\n",
    "* **增加训练数据**：可以有效降低方差；\n",
    "* **正则化（L1/L2）**：在控制模型复杂度时降低方差；\n",
    "* **模型选择**：根据数据复杂度选择合适的模型；\n",
    "* **集成方法（如Bagging, Boosting）**：在一定程度上降低方差，提高泛化能力；\n",
    "* **交叉验证**：用于选择偏差-方差平衡最优点。\n",
    "\n",
    "---\n",
    "\n",
    "# 三、总结笔记版（推荐整理）\n",
    "\n",
    "> **Bias-Variance Decomposition** 将模型误差划分为三部分：偏差（Bias）、方差（Variance）和不可约误差（Irreducible Error）。其中：\n",
    ">\n",
    "> * 偏差衡量模型能力，偏差高说明模型不够复杂（欠拟合）；\n",
    "> * 方差衡量模型稳定性，方差高说明模型对数据过于敏感（过拟合）；\n",
    "> * 不可约误差来自数据本身的噪声，无法通过学习方法减少。\n",
    ">\n",
    "> **Bias-Variance Dilemma（偏差-方差困境）** 指的是偏差和方差通常此消彼长：提高模型复杂度虽然能降低偏差，但可能增加方差；降低方差可能会牺牲模型表达能力，增大偏差。\n",
    ">\n",
    "> 因此，**核心目标是寻找一个偏差与方差的最佳平衡点**，实现最小的泛化误差。这是所有机器学习模型设计与选择的核心原则之一。\n",
    "\n"
   ],
   "id": "6e0e9be32c7a5b13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "677427202c855d17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fc2dbd12003e3203"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
