{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Q1 什么是正则化项？为什么要在损失中添加正则化项？\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 一、什么是正则化项（Regularization Term）？\n",
    "\n",
    "在训练机器学习模型时，我们希望模型不仅在**训练数据**上表现好，也能在**新数据**上泛化良好。但高复杂度模型（如深层神经网络）容易**过拟合（overfitting）**。\n",
    "\n",
    "因此，我们在优化目标中引入一个**正则化项**（regularization term），对模型复杂度进行惩罚，得到如下的优化目标：\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}}(\\theta) = \\mathcal{L}_{\\text{task}}(\\theta) + \\lambda \\cdot \\Omega(\\theta)\n",
    "$$\n",
    "\n",
    "* $\\mathcal{L}_{\\text{task}}(\\theta)$：原始任务损失，如交叉熵、MSE；\n",
    "* $\\Omega(\\theta)$：正则化项，用于惩罚参数复杂度；\n",
    "* $\\lambda$：正则化系数（超参数），控制两者权重。\n",
    "\n",
    "---\n",
    "\n",
    "## 二、为什么要添加正则化项？\n",
    "\n",
    "### 1. 降低过拟合风险\n",
    "\n",
    "通过限制模型的复杂度，让模型不会死记硬背训练数据，而是学习到对整体数据分布有泛化能力的规律。\n",
    "\n",
    "### 2. 控制模型容量\n",
    "\n",
    "添加正则化等价于对模型参数施加约束，例如：\n",
    "\n",
    "* 参数不能太大（L2）；\n",
    "* 参数数量要少（L1）；\n",
    "* 子网络结构随机变化（Dropout）；\n",
    "* 训练时间不应过长（Early Stopping）。\n",
    "\n",
    "### 3. 提升优化稳定性\n",
    "\n",
    "正则化可以改善目标函数的形状，使优化更稳定。例如 L2 会提升函数的**强凸性（strong convexity）**，加快梯度下降收敛。\n",
    "\n",
    "---\n",
    "\n",
    "## 三、常见正则化方法及原理\n",
    "\n",
    "| 正则化方法                    | 数学表达式                                               | 原理与作用              | 特点              |\n",
    "| ------------------------ | --------------------------------------------------- | ------------------ | --------------- |\n",
    "| **L2 正则化（weight decay）** | $\\Omega(\\theta) = \\|\\theta\\|_2^2$                   | 惩罚大权重，使模型更平滑；提升稳定性 | 常用于神经网络权重正则     |\n",
    "| **L1 正则化**               | $\\Omega(\\theta) = \\|\\theta\\|_1$                     | 促使部分参数变为 0，实现稀疏化   | 可用于特征选择         |\n",
    "| **ElasticNet**           | $\\alpha \\|\\theta\\|_1 + (1 - \\alpha) \\|\\theta\\|_2^2$ | 同时稀疏+平滑            | 融合 L1 和 L2      |\n",
    "| **Dropout**              | -                                                   | 随机丢弃部分神经元（训练时）     | 动态平均多个子模型，抑制过拟合 |\n",
    "| **Early Stopping**       | -                                                   | 在验证误差不再下降时停止训练     | 控制训练过程复杂度       |\n",
    "| **数据增强**                 | -                                                   | 扩充训练集的多样性          | 非参数正则方法         |\n",
    "\n",
    "---\n",
    "\n",
    "## 四、Dropout 如何实现正则化？\n",
    "\n",
    "### 原理：\n",
    "\n",
    "在训练时，对每一层的神经元以一定概率 $p$ 随机屏蔽（置为0）。这样模型每次训练都基于一个“随机子网络”。\n",
    "\n",
    "### 机制说明：\n",
    "\n",
    "* 模型每次前向传播都在不同子网络上训练，类似**模型集成（Ensemble）**；\n",
    "* 避免某些神经元形成稳定组合（co-adaptation）；\n",
    "* 测试时使用全网络，但缩放输出为训练时的期望。\n",
    "\n",
    "### 正则化效果：\n",
    "\n",
    "* 抑制训练集过拟合；\n",
    "* 增加模型鲁棒性和泛化能力；\n",
    "* 类似于结构级正则化，不直接出现在损失函数中。\n",
    "\n",
    "---\n",
    "\n",
    "## 五、Early Stopping 如何实现正则化？\n",
    "\n",
    "### 原理：\n",
    "\n",
    "监控验证集误差，如果在连续若干个 epoch 内不再下降，则提前停止训练。\n",
    "\n",
    "### 理解方式：\n",
    "\n",
    "* 训练误差随着训练进行不断下降；\n",
    "* 但验证误差先下降后上升；\n",
    "* 提前停止训练相当于控制了模型学习的“复杂度”；\n",
    "\n",
    "$$\n",
    "\\text{最终模型参数} = \\arg \\min_{\\theta \\text{ at early epoch}} \\mathcal{L}_{\\text{val}}(\\theta)\n",
    "$$\n",
    "\n",
    "### 正则化效果：\n",
    "\n",
    "* 避免过拟合；\n",
    "* 不改变损失函数，但控制优化路径；\n",
    "* 是一种\\*\\*过程级（training process level）\\*\\*正则化方法。\n",
    "\n",
    "---\n",
    "\n",
    "## 六、视觉辅助理解（建议画图）\n",
    "\n",
    "```\n",
    "验证误差\n",
    "   ▲\n",
    "   │         ● ← early stopping\n",
    "   │        ●\n",
    "   │       ●\n",
    "   │     ●\n",
    "   │    ●\n",
    "   │  ●\n",
    "   └────────────────► epoch\n",
    "         训练误差持续下降\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 七、总结归纳\n",
    "\n",
    "| 维度       | L1/L2           | Dropout              | Early Stopping |\n",
    "| -------- | --------------- | -------------------- | -------------- |\n",
    "| 类型       | 参数级正则           | 结构级正则                | 过程级正则          |\n",
    "| 是否显式加入损失 | ✅               | ❌                    | ❌              |\n",
    "| 优点       | 简单、高效           | 强鲁棒性、抗 co-adaptation | 有效避免过拟合        |\n",
    "| 缺点       | 需调节超参 $\\lambda$ | 训练不稳定，测试需缩放          | 验证集质量决定性能      |\n",
    "\n",
    "---\n",
    "\n",
    "## 八、一句话总结\n",
    "\n",
    "> 正则化是机器学习/深度学习中控制模型复杂度、防止过拟合、提升泛化能力的关键机制，既可以通过显式加入损失项（如 L1/L2），也可以通过优化过程控制（如 Dropout、Early Stopping）实现。\n",
    "\n",
    "\n"
   ],
   "id": "2abdce943cd7fd7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "87121e3c3a754534"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6131bcbbdabfb003"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
