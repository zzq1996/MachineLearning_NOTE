{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. 凸函数与非凸函数\n",
    "\n",
    "当然可以，下面我们按照结构化方式详细展开：\n",
    "\n",
    "---\n",
    "\n",
    "# 1. 凸函数与非凸函数\n",
    "\n",
    "## 1.1 凸函数（Convex Function）\n",
    "\n",
    "### 数学定义\n",
    "\n",
    "设 $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$，若对任意 $x, y \\in \\mathbb{R}^n$ 和 $\\lambda \\in [0, 1]$，满足：\n",
    "\n",
    "$$\n",
    "f(\\lambda x + (1 - \\lambda)y) \\leq \\lambda f(x) + (1 - \\lambda)f(y)\n",
    "$$\n",
    "\n",
    "则 $f$ 为**凸函数**。\n",
    "\n",
    "### 图像直观\n",
    "\n",
    "* 函数图像始终在连接两点之间的线段**下方或重合**。\n",
    "* 想象函数图像像一个“碗口向上”的曲线。\n",
    "\n",
    "### 凸函数的性质\n",
    "\n",
    "* 局部最小值一定是全局最小值；\n",
    "* 一阶导数存在时，满足：\n",
    "\n",
    "  $$\n",
    "  f(y) \\geq f(x) + \\nabla f(x)^T (y - x)\n",
    "  $$\n",
    "* 二阶导存在时，Hessian矩阵 $\\nabla^2 f(x)$ 是**半正定的**（特征值全非负）。\n",
    "\n",
    "### 常见凸函数示例\n",
    "\n",
    "* $f(x) = x^2$\n",
    "* $f(x) = e^x$\n",
    "* $f(x) = \\|x\\|_2^2$\n",
    "* 线性函数：既是凸函数也是凹函数\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 非凸函数（Non-convex Function）\n",
    "\n",
    "### 定义\n",
    "\n",
    "如果函数 $f$ 不满足凸函数定义，即存在：\n",
    "\n",
    "$$\n",
    "f(\\lambda x + (1 - \\lambda)y) > \\lambda f(x) + (1 - \\lambda)f(y)\n",
    "$$\n",
    "\n",
    "则为**非凸函数**。\n",
    "\n",
    "### 特点\n",
    "\n",
    "* 存在多个极小值、鞍点；\n",
    "* 优化路径可能困在局部最小值或鞍点；\n",
    "* 函数图像可能有多个波峰波谷，不再“碗口向上”。\n",
    "\n",
    "### 示例\n",
    "\n",
    "* $f(x) = x^4 - 3x^2 + 2$：图像有多个极小值。\n",
    "* 神经网络损失函数：高维、多变量、非线性组合，几乎一定是非凸的。\n",
    "\n",
    "---\n",
    "\n"
   ],
   "id": "560660352d726f20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. 深度学习与凸函数优化\n",
    "\n",
    "## 2.1 理想情况：凸优化\n",
    "\n",
    "许多传统机器学习算法（如线性回归、支持向量机、逻辑回归等）都依赖于**凸优化理论**：\n",
    "\n",
    "| 算法          | 损失函数                | 是否凸函数 |\n",
    "| ----------- | ------------------- | - |\n",
    "| 线性回归        | MSE（平方误差）           | 是 |\n",
    "| Logistic 回归 | 对数似然损失              | 是 |\n",
    "| SVM（二次优化）   | hinge loss + L2 正则项 | 是 |\n",
    "\n",
    "优势在于：\n",
    "\n",
    "* 全局最优可求；\n",
    "* 理论收敛性强；\n",
    "* 易于分析收敛速度和误差界。\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 深度学习：非凸优化问题\n",
    "\n",
    "深度神经网络通过堆叠多个线性层和非线性激活函数（如ReLU）构建复杂模型，其损失函数包含了大量的**参数非线性组合**，整体上是**非凸的**。\n",
    "\n",
    "### 为什么非凸？\n",
    "\n",
    "* 激活函数（ReLU、Sigmoid）引入非线性；\n",
    "* 多层网络结构导致高维耦合；\n",
    "* 网络中存在残差连接、注意力机制等复杂拓扑；\n",
    "* 参数空间极高维，存在**指数级别的局部极小值和鞍点**。\n",
    "\n",
    "### 这意味着什么？\n",
    "\n",
    "* 优化过程可能卡在局部最优；\n",
    "* 初始参数敏感（影响最终结果）；\n",
    "* 学习率和优化器设计更为关键。\n",
    "\n",
    "---\n",
    "\n",
    "## 2.3 如何在非凸中优化？\n",
    "\n",
    "虽然深度学习本质上是非凸优化，但在实践中我们有很多**经验性方法**可以较好地训练神经网络：\n",
    "\n",
    "### 经验对策\n",
    "\n",
    "| 技术                  | 作用             |\n",
    "| ------------------- | -------------- |\n",
    "| 随机梯度下降（SGD）         | 增加扰动，避免陷入局部极小值 |\n",
    "| 动量（Momentum）        | 克服平坦区域         |\n",
    "| Adam / RMSProp      | 自适应学习率，优化收敛路径  |\n",
    "| Dropout / BatchNorm | 改善泛化，稳定训练      |\n",
    "| 残差网络（ResNet）        | 缓解梯度消失，使训练更容易  |\n",
    "| 多次训练+Early Stopping | 选择最优模型，避免过拟合   |\n",
    "\n",
    "### 重要结论\n",
    "\n",
    "尽管神经网络训练是非凸优化问题，但：\n",
    "\n",
    "* 很多局部最小值性能差异不大；\n",
    "* 大规模网络有时甚至**没有陷入坏局部最小值的风险**；\n",
    "* 只要训练技巧得当，最终结果**非常可用**。\n",
    "\n",
    "---\n",
    "\n",
    "## 小结\n",
    "\n",
    "| 项目       | 凸函数  | 非凸函数（深度学习）  |\n",
    "| -------- | ---- | ----------- |\n",
    "| 是否易优化    | 是    | 否           |\n",
    "| 是否全局最优可求 | 是    | 否（可能陷入局部最优） |\n",
    "| 举例       | 线性回归、SVM | 神经网络        |\n",
    "| 应对策略     | 基于理论算法 | 经验+策略优化     |\n",
    "\n"
   ],
   "id": "4fcd8faa98f4ac45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Q1：如何理解“传统机器学习方法依赖凸优化理论”？是不是因为损失函数（例如平方误差）大多是凸函数？但深度学习中有些损失函数也是凸函数，为何深度学习的优化仍是非凸问题？\n",
    "\n",
    "\n",
    "## 一、什么是凸优化？\n",
    "\n",
    "### 1.1 函数凸性定义：\n",
    "\n",
    "设函数 $f(x)$ 在定义域是凸集，那么当它满足下列性质时为**凸函数**：\n",
    "\n",
    "$$\n",
    "f(\\lambda x_1 + (1-\\lambda)x_2) \\leq \\lambda f(x_1) + (1-\\lambda)f(x_2), \\quad \\forall x_1, x_2, \\lambda \\in [0, 1]\n",
    "$$\n",
    "\n",
    "> 直观上：函数图像是“碗形”的，即任意两点之间的连线在函数图像之上。\n",
    "\n",
    "### 1.2 凸优化的优点：\n",
    "\n",
    "* 若目标函数是凸的，且约束也是凸集，则**局部最优即为全局最优**；\n",
    "* 有成熟的理论与算法（梯度下降、牛顿法等）可高效稳定求解；\n",
    "* 可得收敛性保证与泛化性分析。\n",
    "\n",
    "---\n",
    "\n",
    "## 二、传统机器学习与凸优化的关系\n",
    "\n",
    "### 2.1 模型结构简单：\n",
    "\n",
    "例如：\n",
    "\n",
    "* **线性回归**：目标函数为均方误差（MSE）：\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}(w) = \\frac{1}{n} \\sum (y_i - w^\\top x_i)^2\n",
    "  $$\n",
    "* **逻辑回归**：目标函数为对数损失：\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}(w) = -\\sum y_i \\log \\sigma(w^\\top x_i) + (1 - y_i)\\log(1 - \\sigma(w^\\top x_i))\n",
    "  $$\n",
    "\n",
    "这些模型满足：\n",
    "\n",
    "* 损失函数是凸的；\n",
    "* 模型（线性函数）是凸的；\n",
    "* 参数空间是凸集。\n",
    "\n",
    "### 2.2 所以：\n",
    "\n",
    "> 传统机器学习之所以依赖凸优化理论，是因为其模型结构简单，损失函数与参数空间构成**标准凸优化问题**。\n",
    "\n",
    "---\n",
    "\n",
    "## 三、为什么深度学习是非凸优化？\n",
    "\n",
    "即使使用**凸的损失函数**（如 MSE），深度学习的整体优化仍是非凸问题，原因在于：\n",
    "\n",
    "### 3.1 模型结构复杂，导致输出非线性：\n",
    "\n",
    "以一个简单的 2 层网络为例：\n",
    "\n",
    "$$\n",
    "\\hat{y} = W_2 \\cdot \\sigma(W_1 x)\n",
    "$$\n",
    "\n",
    "* $\\sigma$ 是非线性激活函数（如 ReLU）；\n",
    "* 输出是**参数 $W_1, W_2$** 的非线性复合函数。\n",
    "\n",
    "最终的损失函数为：\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(W_1, W_2) = \\sum (y_i - \\hat{y}_i(W_1, W_2))^2\n",
    "$$\n",
    "\n",
    "此时，即便平方项是凸的，复合函数整体：\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(W_1, W_2) = f(g(W_1, W_2))\n",
    "$$\n",
    "\n",
    "由于 $g$ 是非凸的，所以整体也通常是**非凸的**。\n",
    "\n",
    "---\n",
    "\n",
    "## 四、再举类比说明（便于直观理解）\n",
    "\n",
    "考虑：\n",
    "\n",
    "$$\n",
    "f(x) = (1 - \\sin(x))^2\n",
    "$$\n",
    "\n",
    "* $\\sin(x)$：非凸\n",
    "* $(1 - z)^2$：凸函数\n",
    "* 但复合后整体函数是非凸的，含多个局部极小值\n",
    "\n",
    "→ 这与深度神经网络的结构非常相似：**激活函数 + 多层参数组合 = 非凸模型输出**\n",
    "\n",
    "---\n",
    "\n",
    "## 五、非凸优化带来的挑战与实践中为何仍可训练成功？\n",
    "\n",
    "### 5.1 挑战：\n",
    "\n",
    "* 存在多个局部最小值、鞍点、平坦区域；\n",
    "* 优化路径不可预测；\n",
    "* 不同初始化可能得到不同解。\n",
    "\n",
    "### 5.2 实际中可行的原因：\n",
    "\n",
    "* SGD + 动量 + 学习率调度在高维空间中能跳出鞍点；\n",
    "* 网络冗余度高，多个极小值效果相似；\n",
    "* BatchNorm、残差连接等技术提升了可训练性和稳定性；\n",
    "* 神经网络设计经验越来越成熟。\n",
    "\n",
    "---\n",
    "\n",
    "## 总结归纳：\n",
    "\n",
    "| 维度   | 传统机器学习            | 深度学习       |\n",
    "| ---- | ----------------- | ---------- |\n",
    "| 模型结构 | 线性                | 多层非线性嵌套    |\n",
    "| 损失函数 | 通常凸（如MSE、LogLoss） | 可以是凸（如MSE） |\n",
    "| 输出函数 | 关于参数是线性           | 关于参数是非线性   |\n",
    "| 优化目标 | 凸优化问题             | 非凸优化问题     |\n",
    "| 解的性质 | 局部最优即全局最优         | 局部最优，性能相近  |\n",
    "| 理论基础 | 凸优化理论完善           | 非凸优化理论尚不完全 |\n",
    "\n",
    "---\n",
    "\n",
    "## 总结口诀（建议记笔记时保留）：\n",
    "\n",
    "> **深度学习之所以是非凸优化，不在于损失函数是否凸，而在于模型输出对参数的结构是复杂的非线性组合，导致整体目标函数非凸。**\n",
    "\n",
    "\n"
   ],
   "id": "1b19d0e90f4437fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# Q2：如何结合多元函数的梯度理解鞍点（Saddle Point）？\n",
    "\n",
    "---\n",
    "\n",
    "## 一、从多元函数的梯度讲起\n",
    "\n",
    "（多元函数梯度是判断极值点或鞍点的第一步）\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1 多元函数及其梯度（Gradient of Multivariate Function）\n",
    "\n",
    "设有一个多元实值函数（Multivariate Real-valued Function）：\n",
    "\n",
    "$$\n",
    "f: \\mathbb{R}^n \\to \\mathbb{R}, \\quad f(x_1, x_2, \\dots, x_n)\n",
    "$$\n",
    "\n",
    "它的梯度（Gradient）是一个列向量，表示函数在每个方向上的一阶变化率：\n",
    "\n",
    "$$\n",
    "\\nabla f(x) = \\left[ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_n} \\right]^T\n",
    "$$\n",
    "\n",
    "当我们寻找极值点（极小/极大/鞍点）时，第一步就是解方程：\n",
    "\n",
    "$$\n",
    "\\nabla f(x) = 0\n",
    "$$\n",
    "\n",
    "这些解称为**临界点（critical points）**，但注意：\n",
    "**不是所有梯度为0的点都是极值点！**\n",
    "\n",
    "---\n",
    "\n",
    "## 二、鞍点的定义（从梯度角度）\n",
    "\n",
    "### 2.1 鞍点的数学定义回顾（来自前面）：\n",
    "\n",
    "某点 $x^*$ 满足：\n",
    "\n",
    "* 梯度为零（stationary point）：\n",
    "\n",
    "  $$\n",
    "  \\nabla f(x^*) = 0\n",
    "  $$\n",
    "* 但它不是极小值也不是极大值，而是在某些方向上“像最小值”，某些方向上“像最大值”。\n",
    "\n",
    "这就是所谓的鞍点（Saddle Point）。\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 多元函数中鞍点的例子：\n",
    "\n",
    "#### 函数：\n",
    "\n",
    "$$\n",
    "f(x, y) = x^2 - y^2\n",
    "$$\n",
    "\n",
    "#### 梯度：\n",
    "\n",
    "$$\n",
    "\\nabla f(x, y) = \\left[ \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right] = \\left[ 2x, -2y \\right]\n",
    "$$\n",
    "\n",
    "令梯度为0，解得：\n",
    "\n",
    "$$\n",
    "x = 0, \\quad y = 0 \\quad \\Rightarrow \\quad \\text{临界点 } (0, 0)\n",
    "$$\n",
    "\n",
    "#### Hessian 矩阵（二阶导）：\n",
    "\n",
    "$$\n",
    "\\nabla^2 f(x, y) =\n",
    "\\begin{bmatrix}\n",
    "2 & 0 \\\\\n",
    "0 & -2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "其特征值为 $\\lambda_1 = 2 > 0$, $\\lambda_2 = -2 < 0$\n",
    "\n",
    "一个方向上“凸”，一个方向上“凹” ⇒ 鞍点。\n",
    "\n",
    "---\n",
    "\n",
    "## 三、几何解释：为什么这个点不是极小值？\n",
    "\n",
    "以上面函数为例：\n",
    "\n",
    "* 沿 $x$ 轴：函数为 $f(x, 0) = x^2$，是最小值；\n",
    "* 沿 $y$ 轴：函数为 $f(0, y) = -y^2$，是最大值。\n",
    "\n",
    "所以：\n",
    "\n",
    "* 该点虽然梯度为 0，但不同方向呈现不同“曲率”，既不像谷底也不像山顶；\n",
    "* 像**马鞍（saddle）**：前后方向凸起，左右方向凹陷。\n",
    "\n",
    "---\n",
    "\n",
    "## 四、鞍点在高维空间的普遍性（In High Dimensions）\n",
    "\n",
    "在高维深度网络中，函数通常是高度非凸的：\n",
    "\n",
    "* 大量参数 → 高维损失空间；\n",
    "* 梯度为零的点非常多；\n",
    "* **局部最小值较少，鞍点极多**（证明见《Dauphin et al., Identifying and attacking the saddle point problem in high-dimensional non-convex optimization》）；\n",
    "\n",
    "原因：\n",
    "\n",
    "* Hessian 特征值分布在高维中更可能同时出现正负值 ⇒ 鞍点概率远高于极小值。\n",
    "\n",
    "---\n",
    "\n",
    "## 五、深度学习优化中的影响与应对\n",
    "\n",
    "### 5.1 影响：\n",
    "\n",
    "* 训练可能在梯度接近 0 的鞍点附近“卡住”；\n",
    "* 特别是低曲率（flat）鞍点区域，收敛缓慢。\n",
    "\n",
    "### 5.2 应对策略：\n",
    "\n",
    "| 策略        | 英文                              | 解释               |\n",
    "| --------- | ------------------------------- | ---------------- |\n",
    "| 小批量 SGD   | Stochastic Gradient Descent     | 由于梯度有随机波动，容易跳出鞍点 |\n",
    "| 动量法       | Momentum                        | 提高惯性冲出平坦区域       |\n",
    "| 自适应优化器    | Adam, RMSProp                   | 调整每维学习率，提高更新效率   |\n",
    "| 正则化 & 初始化 | Regularization & Initialization | 减少鞍点数量，避免进入不良区域  |\n",
    "| 残差连接      | Residual Connections            | 改变优化地形，缓解深层网络陷阱  |\n",
    "\n",
    "---\n",
    "\n",
    "## 总结（中英对照）\n",
    "\n",
    "| 项目         | 英文术语                 | 内容                    |\n",
    "| ---------- | -------------------- | --------------------- |\n",
    "| 梯度为0点      | Critical Point       | 可能是极小值、极大值或鞍点         |\n",
    "| 鞍点         | Saddle Point         | 梯度为0，但在不同方向上表现为最小/最大值 |\n",
    "| Hessian 矩阵 | Hessian Matrix       | 用于判断曲率，区分临界点类型        |\n",
    "| 特征值混合      | Mixed Eigenvalues    | 出现正负特征值时，说明是鞍点        |\n",
    "| 深度网络中鞍点    | Saddle Points in DNN | 大量存在，需优化器设计应对         |\n",
    "\n",
    "---\n",
    "\n",
    "## 总结口诀：\n",
    "\n",
    "> **多元函数中，梯度为0只是“门槛”，Hessian 才是“裁判”。若特征值正负皆有，鞍点非最小！深度网络中，鞍点虽多，SGD 常能助我们脱困。**\n",
    "\n"
   ],
   "id": "8aae18cc90a364c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fc3699adfd3f4178"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1e5969e29d81282"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a81a4c72e416d0e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dc88aa53fffc1531"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9e09bc97d1fafe76"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
